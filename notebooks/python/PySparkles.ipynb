{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DATA_DIR = '/home/bob/gtd/Iconic/data/samples/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"IconicSparkPy\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(path):\n",
    "    return spark.read.csv(DATA_DIR + path, sep = '\\t', header = True, inferSchema = True)\n",
    "\n",
    "papers = read_dataframe('Papers.txt')\n",
    "authors = read_dataframe('Authors.txt')\n",
    "fos = read_dataframe('FieldsOfStudy.txt')\n",
    "affs = read_dataframe('Affiliations.txt')\n",
    "paa = read_dataframe('PaperAuthorAffiliations.txt')\n",
    "pf = read_dataframe('PaperFieldsOfStudy.txt')\n",
    "pr = read_dataframe('PaperReferences.txt')\n",
    "fc = read_dataframe('FieldOfStudyChildren.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "## Get root field of study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRootFos( fieldId):\n",
    "    filtered = fc.filter(col(\"child\") == fieldId)\n",
    "#     print(filtered.take(1))\n",
    "    if len(filtered.take(1)) == 0:\n",
    "        return fieldId\n",
    "    parent = filtered.take(1)[0][0]\n",
    "    if parent == fieldId:\n",
    "#         print(parent)\n",
    "        return parent\n",
    "    else:\n",
    "#         print('rec')\n",
    "        return computeRootFos( parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12843"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeRootFos(8105449)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add the root field of study to each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[fos: bigint, paper: bigint, similarity: double]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "pf.rdd.map(lambda row : Row(paper = row[0], fos = row[1], similarity = row[2])).toDF()\n",
    "fields = pf.select(\"fos\").distinct().collect()\n",
    "fields = map(lambda field : {'fos' : field[0], 'root' :computeRootFos(field[0])}, fields)\n",
    "fields = list(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = pd.DataFrame(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define helper function to use the root dataframe for getting the root of a field id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRootFos(fos):\n",
    "    return fr[fr[\"fos\"] == 124952713][\"root\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def getRootFos(fos):\n",
    "    return fr[fr[\"fos\"] == fos][\"root\"][0]\n",
    "\n",
    "rootFos_udf = udf(lambda fos : getRootFos(fos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'<lambda>(124952713)'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "124952713"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "rootFos_udf(lit(124952713))\n",
    "getRootFos(124952713)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "RDD is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-0e073f3c486c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mauthorDetails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"paper\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"author\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"paper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"affiliation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffiliation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetRootFos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"author\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"affiliation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountDistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paper\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \"\"\"\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \"\"\"\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m~/programs/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RDD is empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misEmpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: RDD is empty"
     ]
    }
   ],
   "source": [
    "# ego country\n",
    "authorDetails = paa.join(pf, \"paper\")\\\n",
    "    .select(\"author\", \"paper\",\"affiliation\", \"fos\")\\\n",
    "    .rdd.map(lambda row : Row(author = row[0], paper = row[1], affiliation = row[2], fos = getRootFos(row[3]))).toDF()\\\n",
    "    .groupBy(\"author\", \"fos\", \"affiliation\")\\\n",
    "    .agg(F.countDistinct(\"paper\"))\\\n",
    "    .join(MAG.dfAff, col(\"affiliation\") == col(\"id\"))\\\n",
    "    .select(\"author\",\"fos\",\"countryCode\",\"country\")\n",
    "\n",
    "authorDetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtered tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[paper: int, author: bigint, affiliation: bigint]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FILTERED_PAA_YEARS\n",
    "paa = paa.join(papers, col(\"id\") == col(\"paper\"))\\\n",
    "    .filter( (col(\"year\") >= 2007) & (col(\"year\") <= 2017))\\\n",
    "    .select(\"paper\", \"author\", \"affiliation\")\n",
    "paa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[paper: int, fos: bigint, similarity: double, root: string]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PF_WITH_ROOT\n",
    "pf = pf.withColumn(\"root\", rootFos_udf(\"fos\"))\n",
    "pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[paper: int, author: bigint, affiliation: bigint]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fos = 10\n",
    "# FILTERED_PAA_FOS\n",
    "paa = paa.join(pf, \"paper\")\\\n",
    "    .filter(col(\"root\") == fos)\\\n",
    "    .select(\"paper\", \"author\", \"affiliation\")\n",
    "paa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author: bigint, countryCode: string, paper: bigint]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpaa = [(1, 2, \"RO\"),(1,3, \"RO\"),(2,3, \"RO\"),(2,1, \"RO\"),(3,2, \"EN\"),(1,5, \"EN\"),(3,3, \"RO\"),(3,5, \"EN\"),(4,7, \"IT\")]\n",
    "rdd = spark.sparkContext.parallelize(tpaa)\n",
    "rdd = rdd.map(lambda x: Row(paper=int(x[0]), author=int(x[1]), countryCode = x[2]))\n",
    "tpaa = spark.createDataFrame(rdd)\n",
    "tpaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cited: bigint, citing: bigint]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr = [(1,2), (3, 2), (3,4), (4, 1)]\n",
    "rdd = spark.sparkContext.parallelize(paa_rdd)\n",
    "rdd = rdd.map(lambda x: Row(citing=int(x[0]), cited=int(x[1])))\n",
    "tpr = spark.createDataFrame(rdd)\n",
    "tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New table head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGO_country\n",
    "ego_country = paa.join(affs, col(\"affiliation\") == col(\"id\"))\\\n",
    "    .select(\"author\", \"paper\", \"affiliation\", \"countryCode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author: bigint, papers: bigint]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EGO_papers\n",
    "paa.groupBy(\"author\").count().withColumnRenamed(\"count\",\"papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author: bigint, citations: bigint]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EGO_citations\n",
    "paa.join(pr, col(\"paper\") == col(\"cited\"))\\\n",
    "    .groupBy(\"author\")\\\n",
    "    .count().withColumnRenamed(\"count\",\"citations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author: bigint, coauthors: bigint]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EGO_coauthors\n",
    "from pyspark.sql.functions import countDistinct\n",
    "paa.alias(\"1\").join(paa.alias(\"2\"), col(\"1.paper\") == col(\"2.paper\"))\\\n",
    "    .groupBy(\"1.author\")\\\n",
    "    .agg(F.countDistinct(\"2.author\").alias(\"coauthors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author: bigint, count(DISTINCT 2.author): bigint]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EGO_edges - numarul de persoane diferite cu care a colaborat ego ? \n",
    "from pyspark.sql.functions import countDistinct\n",
    "paa.alias(\"1\").join(paa.alias(\"2\"), col(\"1.paper\") == col(\"2.paper\"))\\\n",
    "    .filter( col(\"1.author\") != col(\"2.author\"))\\\n",
    "    .groupBy(\"1.author\")\\\n",
    "    .agg(F.countDistinct(\"2.author\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author: bigint, domestics: bigint]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALTER_domestic\n",
    "paa = ego_country\n",
    "paa.alias(\"1\").join(paa.alias(\"2\"), col(\"1.paper\") == col(\"2.paper\"))\\\n",
    "    .filter(col(\"1.author\") != col(\"2.author\"))\\\n",
    "    .filter(col(\"1.countryCode\") == col(\"2.countryCode\"))\\\n",
    "    .groupBy(\"1.author\")\\\n",
    "    .agg(F.countDistinct(\"2.author\").alias(\"domestics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author: bigint, nondomestics: bigint]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALTER_nondomestic\n",
    "paa = ego_country\n",
    "paa.alias(\"1\").join(paa.alias(\"2\"), col(\"1.paper\") == col(\"2.paper\"))\\\n",
    "    .filter(col(\"1.author\") != col(\"2.author\"))\\\n",
    "    .filter(col(\"1.countryCode\") == col(\"2.countryCode\"))\\\n",
    "    .groupBy(\"1.author\")\\\n",
    "    .agg(F.countDistinct(\"2.author\").alias(\"nondomestics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author: bigint, citations: bigint]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALTER_citations\n",
    "from pyspark.sql.functions import count\n",
    "paa.alias(\"1\").join(paa.alias(\"2\"), col(\"1.paper\") == col(\"2.paper\"))\\\n",
    "    .join(paa.alias(\"3\"), col(\"2.author\") == col(\"3.author\"))\\\n",
    "    .join(pr, col(\"cited\") == col(\"3.paper\"))\\\n",
    "    .groupBy(col(\"1.author\"))\\\n",
    "    .agg(F.count(\"citing\").alias(\"citations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ego: bigint, sum(papers): bigint]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, sum\n",
    "#ALTER_papers - count total number of papers written by coauthors\n",
    "paa.alias(\"1\").join(paa.alias(\"2\"), col(\"1.paper\") == col(\"2.paper\"))\\\n",
    "    .join(paa.alias(\"3\"), col(\"2.author\") == col(\"3.author\"))\\\n",
    "    .filter(col(\"3.paper\") != col(\"1.author\"))\\\n",
    "    .groupBy(col(\"1.author\").alias(\"ego\"), col(\"2.author\").alias(\"alter\"))\\\n",
    "    .agg(F.countDistinct(\"3.paper\").alias(\"papers\"))\\\n",
    "    .groupBy(\"ego\")\\\n",
    "    .agg(F.sum(\"papers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For below, we need a join that takes left elements that do not meet the condition with 0 default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author: bigint, papers_not_alone: bigint]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALTER_EGO_papers\n",
    "paa.alias(\"1\").join(paa.alias(\"2\"),  col(\"1.paper\") == col(\"2.paper\"))\\\n",
    "    .filter(col(\"1.author\") != col(\"2.author\"))\\\n",
    "    .groupBy(\"1.author\")\\\n",
    "    .agg(F.countDistinct(\"2.paper\").alias(\"papers_not_alone\"))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author: bigint, citations: bigint]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALTER_EGO_citations\n",
    "paa.alias(\"1\").join(paa.alias(\"2\"),  col(\"1.paper\") == col(\"2.paper\"))\\\n",
    "    .filter(col(\"1.author\") != col(\"2.author\"))\\\n",
    "    .join(pr, col(\"1.paper\") == col(\"cited\"))\\\n",
    "    .groupBy(\"1.author\")\\\n",
    "    .agg(F.countDistinct(\"citing\").alias(\"citations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author: bigint, countries: bigint]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALTER_country\n",
    "paa.alias(\"1\").join(paa.alias(\"2\"),  col(\"1.paper\") == col(\"2.paper\"))\\\n",
    "    .filter(col(\"1.author\") != col(\"2.author\"))\\\n",
    "    .groupBy(\"1.author\")\\\n",
    "    .agg(F.countDistinct(\"2.countryCode\").alias(\"countries\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ego: bigint, max(papers): bigint]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALTER_MAX_papers\n",
    "paa.alias(\"1\").join(paa.alias(\"2\"), col(\"1.paper\") == col(\"2.paper\"))\\\n",
    "    .join(paa.alias(\"3\"), col(\"2.author\") == col(\"3.author\"))\\\n",
    "    .filter(col(\"1.paper\") != col(\"3.paper\"))\\\n",
    "    .groupBy(col(\"1.author\").alias(\"ego\"), col(\"2.author\").alias(\"alter\"))\\\n",
    "    .agg(F.countDistinct(\"3.paper\").alias(\"papers\"))\\\n",
    "    .groupBy(\"ego\")\\\n",
    "    .agg(F.max(\"papers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author: bigint, max(citations): bigint]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALTER_MAX_citations\n",
    "paa.alias(\"1\").join(paa.alias(\"2\"),  col(\"1.paper\") == col(\"2.paper\"))\\\n",
    "    .filter(col(\"1.author\") != col(\"2.author\"))\\\n",
    "    .join(paa.alias(\"3\"), col(\"2.author\") == col(\"3.author\"))\\\n",
    "    .filter(col(\"1.paper\") != col(\"3.paper\"))\\\n",
    "    .join(pr, col(\"1.paper\") == col(\"cited\"))\\\n",
    "    .groupBy(\"1.author\", \"2.author\")\\\n",
    "    .agg(F.countDistinct(\"citing\").alias(\"citations\"))\\\n",
    "    .groupBy(\"1.author\")\\\n",
    "    .agg(F.max(\"citations\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
